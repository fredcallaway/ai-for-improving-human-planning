{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import matplotlib as plt\n",
    "\n",
    "import itertools as it\n",
    "\n",
    "from analysis_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT = 2\n",
    "data = get_data(EXPERIMENT)\n",
    "figure = Figures(path=f'figs/{EXPERIMENT}', formats=['pdf', 'png']).plot\n",
    "\n",
    "pdf = data['participants'].set_index('pid').copy()\n",
    "mdf = data['mouselab-mdp'].set_index('pid').copy()\n",
    "pdf['feedback'] = pdf.with_feedback.apply({False: 'none', True: 'meta'}.get)\n",
    "mdf['feedback'] = pdf.feedback\n",
    "fb_order = ['none', 'meta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue, orange = sns.color_palette('tab10')[:2]\n",
    "gray = (0.7,)*3\n",
    "palette = {\n",
    "    'none': gray,\n",
    "    'action': blue,\n",
    "    'meta': orange\n",
    "}\n",
    "\n",
    "nice_names = {\n",
    "    'meta': 'Metacognitive FB',\n",
    "    'action': 'Action FB',\n",
    "    'none': 'No FB',\n",
    "    'score': 'Avg. Score (points/trial)',\n",
    "}\n",
    "def reformat_labels(ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    labels = [t.get_text() for t in ax.get_xticklabels()]\n",
    "    new_labels = [nice_names.get(lab, lab) for lab in labels]\n",
    "    ax.set_xticklabels(new_labels)\n",
    "    \n",
    "def reformat_legend(legend):\n",
    "    ax = plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles=handles[1:], labels=[nice_names.get(l, l) for l in labels[1:]])\n",
    "    legend.set_title('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographics and payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.feedback.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.to_numeric(data['survey']['8'], errors='coerce')\n",
    "print(f\"The average age was {age.mean():4.3} ± {age.std():4.3}\")\n",
    "print(f\"Range: {age.min()} – {age.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['total_time'] = data['survey-text'].time_elapsed / 60000\n",
    "pdf['pay'] = (0.75 + pdf.bonus)\n",
    "pdf['wage'] = pdf.pay / (pdf.total_time / 60)\n",
    "print(f'{pdf.wage.mean():.2f}  {pdf.wage.min():.2f}  {pdf.wage.std():.2f}')\n",
    "\n",
    "sns.distplot(pdf.wage)\n",
    "plt.axvline(pdf.wage.mean(), c='r', ls='--')\n",
    "\n",
    "print(f'The average duration of the experiment was {pdf.total_time.mean():.2f} min +/- {pdf.total_time.std():.2f} min.')\n",
    "print(f'The average bonus was ${pdf.bonus.mean():.2f} +/- ${pdf.bonus.std():.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = data['survey-multi-choice'].set_index('pid')\n",
    "responses = pd.DataFrame(list(sdf.responses), index=sdf.index, columns=sdf.questions[0])\n",
    "correct = responses.apply(lambda x: x.mode().iloc[0])  # assume the most common answer is correct\n",
    "errors = responses != correct\n",
    "print('ERROR RATES')\n",
    "print(errors.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_score = mdf.query('block == \"test\"').set_index('feedback').score\n",
    "t_test(test_score.meta, test_score.none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@figure()\n",
    "def plot_test():\n",
    "    sns.barplot('feedback', 'score', data=test_score.reset_index(), palette=palette, order=fb_order)\n",
    "    plt.xlabel('')\n",
    "    #plt.xticks([0,1],['No Feedback','Feedback'])\n",
    "    plt.ylabel('Avg. Score (points/trial)')\n",
    "    plt.title('Test Performance')\n",
    "    reformat_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@figure()\n",
    "def plot_learning():\n",
    "    plt.figure(figsize=(6,3))\n",
    "    g = sns.lineplot(x='trial_index', y='score', hue='feedback',  data=mdf,\n",
    "                     palette=palette, hue_order=fb_order[::-1], \n",
    "    #                  markers='.', dodge=0.3, legend_out=False, aspect=2, \n",
    "                    )\n",
    "\n",
    "    block_changes = mdf.loc[0].block.apply(Labeler()).diff().reset_index().query('block == 1').index\n",
    "    for t in block_changes:\n",
    "        plt.axvline(t-0.5, c='k', ls='--')\n",
    "\n",
    "    plt.xlabel('Trial Number')\n",
    "    plt.ylabel('Score')\n",
    "    legend = plt.gca().legend()\n",
    "    reformat_legend(legend)\n",
    "    plt.xlim(0, 30)\n",
    "    plt.gcf().subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "def bounded_exponential_growth(t, rate, asymptote):\n",
    "    return asymptote * (1 - tt.exp(-(rate / asymptote) * t))\n",
    "\n",
    "def define_model(df, ignore_pr=False):\n",
    "    # Indices and dimensions.\n",
    "    pid = df.pid.as_matrix()  # participant id\n",
    "    n_pid = pid.max() + 1\n",
    "    assert np.all(np.unique(pid) == np.arange(n_pid))\n",
    "    stim = df.stim_i.as_matrix()\n",
    "    n_stim = stim.max() + 1\n",
    "    if ignore_pr:\n",
    "        FB_type = np.zeros(len(df), dtype=int)\n",
    "        n_group = 1\n",
    "    else:\n",
    "        FB_type = df.feedback.apply(fb_order.index).as_matrix()\n",
    "        n_group = len(fb_order)\n",
    "    trial = df.trial_index.as_matrix()\n",
    "    n_trial = trial.max() + 1\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        # Group level parameters.\n",
    "        asymptote_lam = pm.HalfCauchy('asymptote_lam', 10)\n",
    "        rate_lam = pm.HalfCauchy('rate_lam', 10)\n",
    "        asymptote = pm.Exponential('asymptote', asymptote_lam, shape=n_group)\n",
    "        rate = pm.Exponential('rate', rate_lam, shape=n_group)\n",
    "        \n",
    "        # Individual parameters.\n",
    "        pid_baseline = pm.Normal('pid_baseline', 0, 1, shape=n_pid)\n",
    "        \n",
    "        # Stimuli parameters.\n",
    "        sigma = pm.HalfCauchy('sigma', 10., shape=n_stim)\n",
    "        stim_baseline = pm.Normal('baseline', 0, 1, shape=n_stim)\n",
    "        \n",
    "        # Generative model.\n",
    "        t = np.arange(n_trial).reshape(-1, 1)  # broadcasting\n",
    "        improvement = bounded_exponential_growth(t, rate, asymptote)\n",
    "        skill = pid_baseline[pid] + improvement[trial, FB_type]\n",
    "        pred_score = stim_baseline[stim] + skill * sigma[stim]\n",
    "        pm.Normal('score', pred_score, sigma[stim], observed=df.score.as_matrix()) \n",
    "        \n",
    "        pm.Deterministic('improvement', improvement)\n",
    "        pm.Deterministic('pred_score', pred_score)\n",
    "        return model\n",
    "\n",
    "model = define_model(mdf.reset_index())\n",
    "with model:\n",
    "    MAP = pm.find_MAP()\n",
    "    trace = pm.sample(1000, tune=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = ['asymptote', 'rate']\n",
    "# varnames.extend([v+'_lam' for v in varnames])\n",
    "pm.forestplot(trace, varnames=varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz import get\n",
    "comps = list(it.combinations(fb_order[::-1], 2))\n",
    "\n",
    "def compare_params():\n",
    "    for v in varnames:\n",
    "        df = pd.DataFrame(trace[v], columns=fb_order)\n",
    "        for a, b in comps:\n",
    "            diff = df[a] - df[b]\n",
    "            yield {'var': v, 'comp': f'{a} > {b}',  'p_greater': (diff > 0).mean()}\n",
    "            \n",
    "print(pd.DataFrame(compare_params()).set_index(['comp', 'var']).p_greater.unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf['baseline'] = trace['pid_baseline'].mean(0)\n",
    "sns.pointplot('feedback', 'baseline', data=pdf, color='k')\n",
    "sns.swarmplot('feedback', 'baseline', data=pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Skill improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "improvement_est = trace['improvement'].mean(0)\n",
    "trials = range(improvement_est.shape[0])\n",
    "improvement_low, improvement_high = np.rollaxis(pm.hpd(trace['improvement']), -1)\n",
    "\n",
    "idx = pd.MultiIndex.from_product([trials, fb_order],\n",
    "                                 names=['trial_index', 'FB_type'])\n",
    "\n",
    "hdf = pd.DataFrame({\n",
    "    'lower': pd.Series(improvement_low.ravel(), index=idx),\n",
    "    'upper': pd.Series(improvement_high.ravel(), index=idx),\n",
    "    'est': pd.Series(improvement_est.ravel(), index=idx),\n",
    "}).reset_index()\n",
    "\n",
    "g = sns.FacetGrid(hdf, hue='FB_type', aspect=2, size=4)\n",
    "\n",
    "def plot(data, color, label):\n",
    "    offset = (np.arange(3) * 0.1)[fb_order.index(label)]\n",
    "    idx = data.trial_index + offset\n",
    "#     plt.errorbar(idx, data.est, [data.lower, data.upper], c=color, label=label,  elinewidth=1, linewidth=2)\n",
    "    plt.plot(data.trial_index, data.est, c=color, label=label)\n",
    "    plt.fill_between(data.trial_index, data.lower, data.upper, color=color, alpha=0.2)\n",
    "\n",
    "g.map_dataframe(plot)\n",
    "g.add_legend();\n",
    "plt.ylabel('Improvement')\n",
    "# plt.xticks(range(0, 11, 2))\n",
    "plt.xlabel('Trial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_sample, n_trial, n_fb = trace['improvement'].shape\n",
    "idx = pd.MultiIndex.from_product([range(n_sample), range(n_trial), fb_order],\n",
    "                                 names=['sample', 'trial_index', 'FB_type'])\n",
    "sk = pd.DataFrame({'improvement': trace['improvement'].flatten()}, index=idx).improvement.reorder_levels([2,1,0])\n",
    "\n",
    "p_diff = pd.DataFrame({\n",
    "    'Pr(Action FB > No FB)': (sk['action'] > sk['none']).mean(level=[0]),\n",
    "    'Pr(Tutor > No FB)': (sk['meta'] > sk['none']).mean(level=[0]),\n",
    "    'Pr(Tutor > Action FB)': (sk['meta'] > sk['none']).mean(level=[0]),\n",
    "})\n",
    "p_diff.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sig_diff = (p_diff > 0.95)\n",
    "sig_diff_trial = sig_diff.idxmax().apply(lambda x: x if x > 0 else None)  # never becomes significant\n",
    "print('First trial with significant difference in improvement (1 indexed)')\n",
    "print(sig_diff_trial + 1)  # for indexing by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect on strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolz.curried import get\n",
    "\n",
    "test = test.query('block == \"test\"')\n",
    "first_click = test.clicks.apply(get(0, default=None))\n",
    "no_click = first_click.isna()\n",
    "\n",
    "leaves = {5, 6, 7, 9, 10, 11, 16, 17, 18, 20, 21, 22, 27, 28, 29, 31, 32, 33}\n",
    "assert leaves == set(map(int, set(test.path.apply(lambda x: x[-1]))))\n",
    "in_leaves = first_click.isin(leaves)\n",
    "in_leaves[no_click] = np.nan\n",
    "\n",
    "test['first_click_leaf'] = in_leaves\n",
    "test['clicked'] = ~no_click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "from rpy2.robjects.conversion import ri2py\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = test.copy()\n",
    "rdf['feedback'] = (rdf.feedback != 'none').astype(int)\n",
    "rdf['clicked'] = rdf.clicked.astype(int)\n",
    "rdf.first_click_leaf.fillna(0, inplace=True)\n",
    "rdf = rdf[['feedback', 'trial_id', 'first_click_leaf', 'clicked', 'n_clicks', 'score']].reset_index()\n",
    "rdf = rdf.groupby('pid').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i rdf\n",
    "library(lavaan)\n",
    "rdf$pid = factor(rdf$pid)\n",
    "model <- ' \n",
    "# direct effects\n",
    "score ~ a * feedback\n",
    "\n",
    "# indirect effects\n",
    "first_click_leaf ~ b * feedback\n",
    "score ~ c * first_click_leaf\n",
    "\n",
    "clicked ~ d * feedback\n",
    "score ~ e * clicked\n",
    "\n",
    "n_clicks ~ f * feedback\n",
    "score ~ g * n_clicks\n",
    "'\n",
    "fit <- sem(model, data=rdf)\n",
    "summary(fit)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
